Root Cause Analysis
The consistent off-by-one pattern (161â†’160, 246â†’245) plus "very low word counts" in later chapters strongly suggests the bulk queries are still being truncated despite .range().
Why .range(0, 50000) might not work:

Supabase has server-side limits that can override client-side range
The inner join with lemmas!inner might affect row counting
PostgREST (Supabase's API layer) may have configuration limits


The Real Solution
Instead of fighting with bulk queries, let's use parallel chapter queries - still fast but guaranteed accurate:

Claude Code Prompt: Fix Chapter Counts with Parallel Queries (v17)
Task: Replace the broken bulk query approach with parallel per-chapter queries that are both fast AND accurate.
The Problem: Bulk queries with .range(0, 50000) are still being truncated or returning incorrect data. The counts are consistently off.
The Solution: Fetch data for all chapters in parallel using Promise.all. This is still fast (~1-2 seconds) but uses the same proven query logic that worked before the optimization.
Replace the entire fetchChaptersProgress function in src/pages/Dashboard.jsx with:
javascript/**
 * Fetch chapters with progress for carousel - PARALLEL APPROACH
 * Fetches all chapters in parallel for speed, but uses per-chapter queries for accuracy
 *
 * @param {string} userId - User ID
 * @returns {Object} - { chapters, currentChapterIndex, totalChapters }
 */
async function fetchChaptersProgress(userId) {
  console.time('fetchChaptersProgress')

  // Get all chapters first
  const { data: chapters, error: chaptersError } = await supabase
    .from('chapters')
    .select('chapter_id, chapter_number, title')
    .order('chapter_number', { ascending: true })

  if (chaptersError || !chapters || chapters.length === 0) {
    console.error('âŒ [fetchChaptersProgress] chapters query failed:', chaptersError)
    return {
      chapters: [{
        chapter_number: 1,
        title: 'Chapter 1',
        introduced: 0,
        total_lemmas: 0,
        isUnlocked: true,
        isNextToUnlock: false
      }],
      currentChapterIndex: 0,
      totalChapters: 1
    }
  }

  // Get user's introduced lemmas and phrases ONCE (these are small queries)
  const [lemmaProgressResult, phraseProgressResult] = await Promise.all([
    supabase
      .from('user_lemma_progress')
      .select('lemma_id')
      .eq('user_id', userId)
      .gte('reps', 1),
    supabase
      .from('user_phrase_progress')
      .select('phrase_id')
      .eq('user_id', userId)
      .gte('reps', 1)
  ])

  const introducedLemmaIds = new Set((lemmaProgressResult.data || []).map(p => p.lemma_id))
  const introducedPhraseIds = new Set((phraseProgressResult.data || []).map(p => p.phrase_id))

  console.log('ðŸ‘¤ User progress:', { 
    lemmas: introducedLemmaIds.size, 
    phrases: introducedPhraseIds.size 
  })

  // Fetch chapter stats in parallel (all 27 chapters at once)
  const chapterStatsPromises = chapters.map(async (chapter) => {
    // Get sentences for this chapter
    const { data: sentences } = await supabase
      .from('sentences')
      .select('sentence_id')
      .eq('chapter_id', chapter.chapter_id)

    const sentenceIds = (sentences || []).map(s => s.sentence_id)

    if (sentenceIds.length === 0) {
      return {
        chapter_number: chapter.chapter_number,
        title: chapter.title,
        introduced: 0,
        total_lemmas: 0,
        lemmaCount: 0,
        phraseCount: 0
      }
    }

    // Get lemmas (excluding stop words) and phrases in parallel
    const [wordsResult, phrasesResult] = await Promise.all([
      supabase
        .from('words')
        .select('lemma_id, lemmas!inner(is_stop_word)')
        .in('sentence_id', sentenceIds)
        .eq('lemmas.is_stop_word', false),
      supabase
        .from('phrase_occurrences')
        .select('phrase_id')
        .in('sentence_id', sentenceIds)
    ])

    // Get unique lemmas and phrases for this chapter
    const chapterLemmaIds = [...new Set((wordsResult.data || []).map(w => w.lemma_id))]
    const chapterPhraseIds = [...new Set((phrasesResult.data || []).map(po => po.phrase_id))]

    // Count introduced
    const introducedLemmaCount = chapterLemmaIds.filter(id => introducedLemmaIds.has(id)).length
    const introducedPhraseCount = chapterPhraseIds.filter(id => introducedPhraseIds.has(id)).length

    return {
      chapter_number: chapter.chapter_number,
      title: chapter.title,
      introduced: introducedLemmaCount + introducedPhraseCount,
      total_lemmas: chapterLemmaIds.length + chapterPhraseIds.length,
      lemmaCount: chapterLemmaIds.length,
      phraseCount: chapterPhraseIds.length
    }
  })

  // Wait for all chapter stats
  const chapterStats = await Promise.all(chapterStatsPromises)

  // Debug: Log first 3 chapters
  console.log('ðŸ“Š Chapter stats sample:', chapterStats.slice(0, 3).map(c => ({
    ch: c.chapter_number,
    total: c.total_lemmas,
    lemmas: c.lemmaCount,
    phrases: c.phraseCount,
    introduced: c.introduced
  })))

  // Build final chapters array with unlock logic
  const chaptersWithProgress = []
  let previousChapterComplete = true

  for (const stats of chapterStats) {
    const completionRate = stats.total_lemmas > 0 ? stats.introduced / stats.total_lemmas : 0

    const isUnlocked = stats.chapter_number === 1 || previousChapterComplete
    const isNextToUnlock = !isUnlocked && previousChapterComplete

    chaptersWithProgress.push({
      chapter_number: stats.chapter_number,
      title: stats.title,
      introduced: stats.introduced,
      total_lemmas: stats.total_lemmas,
      isUnlocked,
      isNextToUnlock,
      completionRate
    })

    // Update for next iteration (95% threshold)
    previousChapterComplete = completionRate >= 0.95
  }

  // Find current chapter: first unlocked with < 100% completion
  let currentChapterIndex = chaptersWithProgress.findIndex(
    ch => ch.isUnlocked && ch.completionRate < 1.0
  )

  if (currentChapterIndex === -1) {
    currentChapterIndex = 0
  }

  console.timeEnd('fetchChaptersProgress')
  console.log(`ðŸ“š [fetchChaptersProgress] Current chapter: ${currentChapterIndex + 1}, Total: ${chapters.length}`)

  // Return with "1 back, current, 2 forward" logic for visible chapters
  const startIndex = Math.max(0, currentChapterIndex - 1)
  const endIndex = Math.min(chaptersWithProgress.length, startIndex + 4)
  const adjustedStartIndex = Math.max(0, endIndex - 4)

  const visibleChapters = chaptersWithProgress.slice(adjustedStartIndex, adjustedStartIndex + 4)
  const visibleCurrentIndex = currentChapterIndex - adjustedStartIndex

  return {
    chapters: visibleChapters,
    currentChapterIndex: visibleCurrentIndex,
    totalChapters: chapters.length,
    _allChaptersCache: chaptersWithProgress,
    _absoluteCurrentIndex: currentChapterIndex
  }
}
```

### Expected Console Output
```
ðŸ‘¤ User progress: { lemmas: 155, phrases: 23 }
ðŸ“Š Chapter stats sample: [
  { ch: 1, total: 161, lemmas: 141, phrases: 20, introduced: 161 },
  { ch: 2, total: 246, lemmas: 206, phrases: 40, introduced: 68 },
  { ch: 3, total: 156, lemmas: 123, phrases: 33, introduced: 28 }
]
fetchChaptersProgress: ~1500ms
ðŸ“š [fetchChaptersProgress] Current chapter: 2, Total: 27
Why This Will Work
ApproachQueriesAccuracySpeedOriginal (sequential)27 Ã— 3 = 81âœ… CorrectâŒ ~15 secBulk queries6âŒ Truncatedâœ… ~1 secParallel1 + 27Ã—2 = 55âœ… Correctâœ… ~1.5 sec
The parallel approach runs all 27 chapter queries simultaneously, so total time â‰ˆ slowest single query, not sum of all queries.
Verification
After implementing, the console should show:

Chapter 1: total: 161 (141 lemmas + 20 phrases)
Chapter 2: total: 246 (206 lemmas + 40 phrases)

If these match, the fix is working correctly.

This replaces the broken bulk query approach with a proven parallel approach that's both fast and accurate.